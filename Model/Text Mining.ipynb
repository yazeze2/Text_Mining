{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Model - Topic Modeling and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import lda\n",
    "import numpy as np\n",
    "import lda.datasets\n",
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Nov 18 23:59:58 +0000 2016</td>\n",
       "      <td>arunprasad72</td>\n",
       "      <td>RT @Praveen_1singh: First the stone pelting st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Nov 18 23:59:49 +0000 2016</td>\n",
       "      <td>pranavkisu</td>\n",
       "      <td>RT @NewDelhiTimesIN: Is the #demonetization of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Nov 18 23:59:48 +0000 2016</td>\n",
       "      <td>bablumohan</td>\n",
       "      <td>RT @scoopwhoopnews: #BREAKING Banks across Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Nov 18 23:59:37 +0000 2016</td>\n",
       "      <td>NagrathRob</td>\n",
       "      <td>RT @DrGPradhan: .@ravishndtv of @ndtv spreadin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Nov 18 23:59:28 +0000 2016</td>\n",
       "      <td>ManishPrasa</td>\n",
       "      <td>RT @YesIamSaffron: जब भी #Demonetization व् का...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at   screen_name  \\\n",
       "0  Fri Nov 18 23:59:58 +0000 2016  arunprasad72   \n",
       "1  Fri Nov 18 23:59:49 +0000 2016    pranavkisu   \n",
       "2  Fri Nov 18 23:59:48 +0000 2016    bablumohan   \n",
       "3  Fri Nov 18 23:59:37 +0000 2016    NagrathRob   \n",
       "4  Fri Nov 18 23:59:28 +0000 2016   ManishPrasa   \n",
       "\n",
       "                                                text  \n",
       "0  RT @Praveen_1singh: First the stone pelting st...  \n",
       "1  RT @NewDelhiTimesIN: Is the #demonetization of...  \n",
       "2  RT @scoopwhoopnews: #BREAKING Banks across Ind...  \n",
       "3  RT @DrGPradhan: .@ravishndtv of @ndtv spreadin...  \n",
       "4  RT @YesIamSaffron: जब भी #Demonetization व् का...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv', sep=\"\\t\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modelling (LDA - Latent Dirichlet allocation)\n",
    "\n",
    "In natural language processing, Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer(input='content', stop_words='english', min_df=1)\n",
    "df.text.fillna('', inplace=True)\n",
    "dtm = vectorizer.fit_transform(df.text).toarray()\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '00f8drqczr', '025', '039mojsvyi', '0480654',\n",
       "       '04bevf5lre', '06', '07', '08', '09ip2atmbg', '09mitali',\n",
       "       '09uv9sfd2y', '0bfujsgltb', '0bwfwuirqk', '0fan6b2wxv',\n",
       "       '0fmh3umbvq', '0frhtzuyhv', '0funzj0fjo', '0i47zl55f2'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 11914\n",
      "INFO:lda:vocab_size: 11019\n",
      "INFO:lda:n_words: 141760\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -1372721\n",
      "INFO:lda:<10> log likelihood: -978602\n",
      "INFO:lda:<20> log likelihood: -954704\n",
      "INFO:lda:<30> log likelihood: -950702\n",
      "INFO:lda:<40> log likelihood: -947126\n",
      "INFO:lda:<50> log likelihood: -944809\n",
      "INFO:lda:<60> log likelihood: -943443\n",
      "INFO:lda:<70> log likelihood: -942278\n",
      "INFO:lda:<80> log likelihood: -940852\n",
      "INFO:lda:<90> log likelihood: -939839\n",
      "INFO:lda:<100> log likelihood: -939844\n",
      "INFO:lda:<110> log likelihood: -938514\n",
      "INFO:lda:<120> log likelihood: -938177\n",
      "INFO:lda:<130> log likelihood: -936859\n",
      "INFO:lda:<140> log likelihood: -936467\n",
      "INFO:lda:<150> log likelihood: -936332\n",
      "INFO:lda:<160> log likelihood: -935451\n",
      "INFO:lda:<170> log likelihood: -935602\n",
      "INFO:lda:<180> log likelihood: -935251\n",
      "INFO:lda:<190> log likelihood: -935235\n",
      "INFO:lda:<200> log likelihood: -934997\n",
      "INFO:lda:<210> log likelihood: -934803\n",
      "INFO:lda:<220> log likelihood: -934738\n",
      "INFO:lda:<230> log likelihood: -934730\n",
      "INFO:lda:<240> log likelihood: -934369\n",
      "INFO:lda:<250> log likelihood: -934380\n",
      "INFO:lda:<260> log likelihood: -934140\n",
      "INFO:lda:<270> log likelihood: -933980\n",
      "INFO:lda:<280> log likelihood: -933979\n",
      "INFO:lda:<290> log likelihood: -934047\n",
      "INFO:lda:<300> log likelihood: -933880\n",
      "INFO:lda:<310> log likelihood: -933635\n",
      "INFO:lda:<320> log likelihood: -934069\n",
      "INFO:lda:<330> log likelihood: -933979\n",
      "INFO:lda:<340> log likelihood: -933782\n",
      "INFO:lda:<350> log likelihood: -933873\n",
      "INFO:lda:<360> log likelihood: -934272\n",
      "INFO:lda:<370> log likelihood: -933134\n",
      "INFO:lda:<380> log likelihood: -933777\n",
      "INFO:lda:<390> log likelihood: -933663\n",
      "INFO:lda:<400> log likelihood: -933632\n",
      "INFO:lda:<410> log likelihood: -933780\n",
      "INFO:lda:<420> log likelihood: -933715\n",
      "INFO:lda:<430> log likelihood: -933745\n",
      "INFO:lda:<440> log likelihood: -933611\n",
      "INFO:lda:<450> log likelihood: -933402\n",
      "INFO:lda:<460> log likelihood: -933443\n",
      "INFO:lda:<470> log likelihood: -933286\n",
      "INFO:lda:<480> log likelihood: -933744\n",
      "INFO:lda:<490> log likelihood: -933609\n",
      "INFO:lda:<499> log likelihood: -933638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x26d4f1be0b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings                  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "text = df.text\n",
    "model = lda.LDA(n_topics=5, n_iter=500, random_state=1)\n",
    "model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.64296203e-07, 3.64296203e-07, 3.67939165e-05, ...,\n",
       "        3.64296203e-07, 3.64296203e-07, 3.64296203e-07],\n",
       "       [3.99357992e-07, 5.59500547e-04, 3.99357992e-07, ...,\n",
       "        3.99357992e-07, 3.99357992e-07, 8.02709564e-05],\n",
       "       [3.72686687e-07, 1.86716030e-04, 3.72686687e-07, ...,\n",
       "        3.72686687e-07, 3.72686687e-07, 3.72686687e-07],\n",
       "       [7.48101351e-05, 4.22764252e-04, 2.48538655e-07, ...,\n",
       "        2.48538655e-07, 2.48538655e-07, 2.48538655e-07],\n",
       "       [4.39498813e-07, 4.39498813e-07, 4.39498813e-07, ...,\n",
       "        1.32289143e-04, 2.20188905e-04, 4.39498813e-07]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the key words that come together for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: demonetization rt https purpose devyanidilli mouth uses\n",
      "Topic 1: demonetization rt https sardesairajdeep taken congress struggling\n",
      "Topic 2: demonetization rt https पर कर रह modi\n",
      "Topic 3: demonetization https rt hai modi bank nahi\n",
      "Topic 4: demonetization rt https money black anna govt\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Topic for each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 4 , RT @Praveen_1singh: First the stone pelting stopped and now this!! \r\n",
      "What months of politics and talks couldn't do #demonetization did in da…\n",
      "topic: 0 , RT @NewDelhiTimesIN: Is the #demonetization of ₹1000 &amp; ₹500 notes good for India? \r\n",
      "\r\n",
      "@AmitShah @OfficeOfRG @PMOIndia @BJP4India\n",
      "topic: 0 , RT @scoopwhoopnews: #BREAKING Banks across India to serve only senior citizens tomorrow: NDTV\r\n",
      "#demonetization\n",
      "topic: 1 , RT @DrGPradhan: .@ravishndtv of @ndtv spreading rumours to provoke people against #demonetization &amp; PM @narendramodi \r\n",
      "\r\n",
      "He need mob treatmen…\n",
      "topic: 2 , RT @YesIamSaffron: जब भी #Demonetization व् काली धन का इतिहास लीखा जाएगा फ़र्ज़ीवाल का नाम सबसे ऊपर काले अछर से लिखा जाएगा @ArvindKejriwal…\n",
      "topic: 3 , Agree Sir reason of worry for SC could b some or all of them were affected by #demonetization or instructions by ma… https://t.co/ifqDuDbcEm\n",
      "topic: 3 , @BspUp2017 @OfficeOfRG @MamataOfficial @ArvindKejriwal  #सारे_चोर_मचाये_शोर  #demonetization  #ModiFightsCorruption \r\n",
      "https://t.co/4ElMIvgygX\n",
      "topic: 1 , RT @janlokpal: Reaction of Aam Aadmi Party leader Ashutosh when he learned about #demonetization https://t.co/OepaLUM56L\n",
      "topic: 3 , RT @RoflGandhi_: When the bank manager says kal aana, cash nahi hai. #demonetization https://t.co/DRyGmtoX7x\n",
      "topic: 3 , RT @ANI_news: Aam Aadmi Party naraz ho sakti hai, par aam aadmi naraz nahi hai: Union Minister Venkaiah Naidu #demonetization https://t.co/…\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for n in range(10):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"topic: {} , {}\".format(topic_most_pr,text[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author when writing), or the intended emotional communication (that is to say, the emotional effect the author wishes to have on the reader).\n",
    "\n",
    "A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level — whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. We will use knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = []\n",
    "neg_features = []\n",
    "def make_full_dict(word):\n",
    "    return dict([(word, True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive-words.txt','r', encoding='utf-8') as posFile:\n",
    "    lines = posFile.readlines()\n",
    "    for line in lines:\n",
    "        pos_features.append([make_full_dict(line.rstrip()),'pos'])\n",
    "        \n",
    "with open('negative-words.txt','r', encoding='utf-8') as negFile:\n",
    "    lines = negFile.readlines()\n",
    "    for line in lines:\n",
    "        neg_features.append([make_full_dict(line.rstrip()),'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8021, 4942)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_features),len(neg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = pos_features + neg_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "referenceSets = collections.defaultdict(set)\n",
    "testSets = collections.defaultdict(set)\n",
    "def make_full_dict_sent(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Daily', 'Mail', 'stole', 'My', 'Visualization', ',', 'Twice']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "neg_test = 'I hate data science'\n",
    "title_words = re.findall(r\"[\\w']+|[.,!?;]\",\n",
    "                         'The Daily Mail stole My Visualization, Twice')\n",
    "title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{',': True,\n",
       "   'Daily': True,\n",
       "   'Mail': True,\n",
       "   'My': True,\n",
       "   'The': True,\n",
       "   'Twice': True,\n",
       "   'Visualization': True,\n",
       "   'stole': True},\n",
       "  '']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=[]\n",
    "test.append([make_full_dict_sent(title_words),''])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Test Classifier\n",
    "\n",
    "for i, (features, label) in enumerate(test):\n",
    "    predicted = classifier.classify(features)\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos RT @Praveen_1singh: First the stone pelting stopped and now this!! \r\n",
      "What months of politics and talks couldn't do #demonetization did in da…\n",
      "pos RT @NewDelhiTimesIN: Is the #demonetization of ₹1000 &amp; ₹500 notes good for India? \r\n",
      "\r\n",
      "@AmitShah @OfficeOfRG @PMOIndia @BJP4India\n",
      "neg RT @scoopwhoopnews: #BREAKING Banks across India to serve only senior citizens tomorrow: NDTV\r\n",
      "#demonetization\n",
      "neg RT @DrGPradhan: .@ravishndtv of @ndtv spreading rumours to provoke people against #demonetization &amp; PM @narendramodi \r\n",
      "\r\n",
      "He need mob treatmen…\n",
      "pos RT @YesIamSaffron: जब भी #Demonetization व् काली धन का इतिहास लीखा जाएगा फ़र्ज़ीवाल का नाम सबसे ऊपर काले अछर से लिखा जाएगा @ArvindKejriwal…\n",
      "neg Agree Sir reason of worry for SC could b some or all of them were affected by #demonetization or instructions by ma… https://t.co/ifqDuDbcEm\n",
      "pos @BspUp2017 @OfficeOfRG @MamataOfficial @ArvindKejriwal  #सारे_चोर_मचाये_शोर  #demonetization  #ModiFightsCorruption \r\n",
      "https://t.co/4ElMIvgygX\n",
      "pos RT @janlokpal: Reaction of Aam Aadmi Party leader Ashutosh when he learned about #demonetization https://t.co/OepaLUM56L\n",
      "pos RT @RoflGandhi_: When the bank manager says kal aana, cash nahi hai. #demonetization https://t.co/DRyGmtoX7x\n",
      "pos RT @ANI_news: Aam Aadmi Party naraz ho sakti hai, par aam aadmi naraz nahi hai: Union Minister Venkaiah Naidu #demonetization https://t.co/…\n",
      "pos RT @MinhazMerchant: Anna Hazare tells @CNNnews18 #demonetization is \"a historic decision by govt, those with black money against move.\" And…\n",
      "pos Happy Sauce #TheGrandTour #OptOutside #ChildrenInNeed #ReasonsToSkipThanksgiving #MakeAFilmFeelUncertain #demonetization #FridayFeeling\n",
      "neg RT @bhaiyyajispeaks: Here @sardesairajdeep struggling for one answer against #Demonetization, He should have taken some Congress leader lik…\n",
      "pos RT @Coffeeliciious: Hey black money holders,  save your ass asap... I mean assets. Now you will be in jail soon. 😂😂😂\r\n",
      "\r\n",
      "#demonetization \r\n",
      " #Se…\n",
      "pos RT @jgopikrishnan70: Then why not 1000 notes also....really hit by #demonetization https://t.co/k37rKRQip7\n",
      "neg @TVMohandasPai has anyone asked how many have died due to #BlackMoney since 1947. #demonetization\n",
      "neg RT @bhaiyyajispeaks: Here @sardesairajdeep struggling for one answer against #Demonetization, He should have taken some Congress leader lik…\n",
      "pos RT @MumCongress: In last 9 days economy of Mumbai, hits rock bottoms due to #Demonetization. #BlackMoney https://t.co/XC7X6WNcje\n",
      "pos RT @RealHistoryPic: Can't say anything about Productivity but Creativity has increased after #Demonetization.(2016) https://t.co/0fMh3UMbVq\n",
      "pos RT @harshkkapoor: मिले सुर मेरा तुम्हारा\r\n",
      "तो सुर मिले हमारा\r\n",
      "#DeMonetization\r\n",
      "Friends let's defeat corrupt Hoarders of Blackmoney\r\n",
      "https://t.co…\n"
     ]
    }
   ],
   "source": [
    "for doc in df.text[:20]:\n",
    "    title_words = re.findall(r\"[\\w']+|[.,!?;]\", doc.lower())\n",
    "    test = []\n",
    "    test.append([make_full_dict_sent(title_words),''])\n",
    "    for i, (features, label) in enumerate(test):\n",
    "        predicted = classifier.classify(features)\n",
    "        print(predicted,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As can be seen from the results above (on the first 20 tweets), our model performed well in the topic modeling task as well as sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
